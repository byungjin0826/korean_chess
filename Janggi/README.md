# 구 버전 설명

야매장기는 알파고에서 강하게 영감을 받아 딥러닝으로 구현한 장기 AI입니다. 딥러닝은 텐서플로우로 구현되었고, 나머지 게임과 MinMax, 강화 학습 등은 WPF 및 C#으로 되 어 있습니다. 프로젝트 시작한 지는 너무 오래됐는데 제가 관리를 안 해서 점점 시작한 의미가 없어지고 있네요. 앞으로도 별로 건드려볼 생각은 없습니다.. ㅎㅎ

## Environment / Requirement

 - Visual Studio 2017
 - .Net Framework 4.7
 - Windows 10
 - Python 3.6 64bit
 - TensorFlow

모든 개발은 Visual Studio 2017에서 이루어졌습니다. C#/WPF 코드는 .NET 4.7 환경에서 만들었습니다. TensorFlow를 이용한 딥러닝 알고리즘은 Python 3.6에서 작동됩니다.
본래 학습을 위한 코드는 파이썬 단독으로 이루어지는 것이 마땅하나 레인포스 러닝을 빠르게 돌리기 위해서 결국 C#과 하이브리드 형태로 만들게 되었습니다.

## 프로젝트 구성

 - genMoveSet : 별 거 아닌 보조 프로젝트. 장기판에서 움직일 수 있는 모든 경우의 수, 2450가지의 움직임을 생성합니다. 여기서 생성된 결과를 소스 코드에 복사해서 사용했.. 었는데 최종적으로는 쓰지 않아요.
 - Janggi : 메인 알고리즘이 구현된 프로젝트입니다.
 - RunnerConsole : 콘솔 환경에서 실행하는 것들이 포함되어있습니다. 간단한 테스트에서부터 지도학습, 강화학습 등이 포함됩니다.
 - RunnerWpf : GUI환경에서 동작합니다. 장기를 직접 두어볼 수 있고, 컴퓨터의 생각 트리를 확인할 수 있습니다.
 - TensorNetworks : 텐서 플로우 프로젝트입니다. TCP/IP로 Janggi 라이브러리와 통신합니다.

## 학습 방법

### 기보 파일

기보 파일은 본 깃허브의 릴리즈에 올려놓았습니다. 장기도사 사이트 게시판에서 긁어온 겁니다. 장기도사는 또 어딘가에서 가져온 것이구요..

### 기보 파일 경로 지정

파라미터를 받을 수 있게 해놔야 정상인데 뭐 VS에서는 그냥 코드 고치는게 편해서 ㅋㅋ 모든 경로는 하드코딩되어 있습니다. RunnerConsole프로젝트에서 Process/Train.cs 를 찾아보시고.. 거기서 *.gib 로 검색하시면 genGibo 함수 내부에서 기보 파일이 들어있는 경로를 설정할 수 있습니다.

### 학습 방법 지정

RunnerConsole에서 Process/Train.cs의 Train() 함수를 보시면 세 가지 테스크로 나뉩니다. 첫 번째는 데이터 생성, 두 번째는 증강, 세 번째는 학습인데.. 데이터 생성에서 genGibo() 함수는 기보로부터 학습 파일을 불러오는 것이고.. genReignforce()는 자체 경기를 통해 학습 데이터를 생성합니다.

### 학습 모델 지정

학습 모델은 TensorNetworks라는 파이썬 프로젝트에서 결정합니다. _params.py 파일을 보시면 정책 네트워크와 밸류 네트워크의 모델을 고를 수 있습니다. tensor_network.py와 value_network.py 에는 각각 get_netwrok함수가 있는데, 여기서 문자열을 받아와서 네트워크를 고릅니다.

파이썬은 TCP로 요청을 받아서 동작합니다. 주요 동작으로는 모델 로드(혹은 이닛), training, evaluation, save 이렇게 네 가지 동작이 제공되며 이에 대한 자세한 구현은 tensor_networks.py에 들어있습니다. tcp_comm_server.py에서 명령을 받아와서 함수를 실행합니다. 주고 받는 데이터에 대한 프로토콜 포맷도 여기 파일에 기술되어 있습니다.

### 학습 실행

RunnerConsole과 TensorNetworks를 동시에 실행하셔야 합니다. 솔루션 익스플로러에서 솔루션에 대고 오른쪽 클릭 -> 속성 -> Common -> Start up ... 등등 가시면 설정할 수 있습니다.

모든 프로세스 진행은 C#이 담당하고 파이썬은 단지 네트워크 모듈로서만 작동합니다. C#에 타이머를 걸어놓아서 10분에 한 번씩 저장 명령을 날립니다.

텐서보드는 log 폴더를 참고하세요.

### 학습 결과 활용

결과를 보시려면 RunnerWpf와 TensorNetworks 를 동시에 실행합니다. RnnnerWpf의 MainWikndows.xaml.cs 에서 'new Mcts'로 검색해보세요. Mcts는 각종 변형이 있을 수 있는데 이와 관련된 함수를 인자로 받아옵니다. realYame는 가장 보통의 Mcts 구현이고.... onlyPolicy는 정책 네트워크만 사용한 구현입니다.

오른쪽에서 Mcts를 트리구조로 탐색할 수 있고, 각 노드의 장기판을 볼 수도 있습니다. 요거 하나 그나마 쓸만한 거 같네요 ㅋㅋ

## 네트워크 간단 소개

resnet이라고 구현해 놓은 것은 SE 적용된 버전을 올려놨습니다. 장기 학습에는 오히려 단순한 VGG스타일이 좋을 것 같기도 하네요.

네트워크의 입력은 현 보드의 상황이고, 출력은 이길 확률(Value Network) 혹은 어디를 둬야 할지(Policy Network) 입니다. 현재의 장기판을 어떻게 매트릭스로 표현하는지는 Janggi프로젝트에서 Board.cs 의 GetBytes를 보면 확인할 수 있는데요, 중요한 것은 항상 아래쪽이 둘 차례라는 것이며 한인지 초인지는 상관하지 않습니다. (코드 내부의 변수명은 아래쪽이 초 혹은 my로 되어 있습니다.)

Policy 네트워크에서는 어디에 둔다는 행위 (x1,y1) -> (x2,y2) 또한 매트릭스로 표현해야 합니다. 10x9의 출발점 채널과 도착점 채널로 나뉘어 표현하고, 각 채널에 대해 따로 Softmax를 적용합니다. 출발점과 도착점의 모든 조합에 대해서 출발점 확률 P1(x1,y1) * 도착점 확률 P2(x2,y2) 로 계산하면 해당 Move에 대한 확률이 됩니다. 모든 경우의 수를 조사할 필요는 없고, 현재 보드에서 가능한 움직임 30여가지에 대해서만 계산하면 됩니다. 

여기에는 약간 꺼림직한 부분이 있습니다. 출발점의 가장 높은 값과 도착점의 가장 높은 값, 그 둘을 연결하는 것이 과연 맞느냐.. 출발점은 내 돌, 도착점은 상대 돌이나 빈 공간을 뜻하는데요, 예를 들어 도착점의 확률을 보니 상대방의 병을 먹는 것이 가장 높은 확률로 나왔다고 칩시다. 그런데 여기서 뭘로 먹는지는 안 알려주거든요. 출발점의 확률을 보니 내 차를 움직이라는 겁니다. 그런데 만약 차로는 병을 못 먹는다? 그럴 수 있지 않겠습니까?

단순히 생각하면 출발점(90) * 도착점(90)가지의 움직임이 가능할 것 같지만 실제로는 그렇지 않습니다. 예를 들어 대각선으로 이동하는 돌은 장기에서 없기 때문에 그런 움직임은 고려할 필요가 없죠. 그래서 모든 경우의 수를 계산해보니 26xx 얼마더라 하여튼 그 정도 나옵니다. 그래서 모든 경우의 수에 대해 일일이 확률을 매겼었습니다. 이렇게 하면 위의 말한 문제가 해결되겠죠. 그런데 뭐.. 그냥 복잡하더라구요. 학습이 잘 되서 단 한 가지 완벽한 정답을 네트워크가 알고 있다면 그냥 다 상관없을 것 같고.. 설명하기 복잡한 얘긴데 하여튼 그렇습니다.

Value 네트워크의 출력은 엄청 간단합니다. 결과가 1이면 아래쪽이 이기는 것이고 0이면 위쪽이 이기는 것이죠.
